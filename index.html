<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sai Mitheran J</title>
  
  <meta name="author" content="Sai Mitheran">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/saimitheran.png">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="border-radius:30px;margin-top:15px;margin-bottom:15px;background-color:#e9ecef;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sai Mitheran Jagadesh Kumar</name>
              </p>
              <p>I am a second-year Master's student at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, majoring in Electrical and Computer Engineering, specializing in AI/ML systems. I graduated from <a href="https://www.nitt.edu/">National Institute of Technology, Tiruchirappalli</a> in 2022 as a <b>Gold medalist</b> in Electronics and Communication Engineering.
                I was affiliated with the <a href="https://www.mpi-inf.mpg.de/home">Max Planck Institute of Informatics</a>, Saarbr√ºcken, funded by the <a href = "https://www2.daad.de/deutschland/stipendium/datenbank/en/21148-scholarship-database/?detail=50015295">DAAD-WISE Scholarship</a>.
              </p>
              
                    <hr style="border:0;border-bottom-color:#ccc;border-bottom-style:solid;border-bottom-width: 1px;padding-top:10px;">
             
              <p>
                I'm a recipient of the <a href = "https://webjapps.ias.ac.in/fellowship2021/lists/selectedList.jsp">Indian Academy of Sciences Research Fellowship</a>, the prestigious <a href = "https://www.nitt.edu/home/ala_award.pdf">Dr. A.L. Abdussattar Memorial Award</a>, the <a href = "https://www.nitt.edu/home/academics/iday/institute_day_2023/ENDOWMENT-AWARDS-LIST-2023.pdf">Sri Janardhana Iyengar Memorial Award</a>, and the <a href = "http://www.replicabilitystamp.org/">Graphics Replicability Stamp Award</a>.
                Outside of research, I'm open to anything to do with Sustainability and Mental Health. Scroll down to know more!
                
              </p>  
              
              <p style="text-align:center">
                <a href="https://linktr.ee/smj007">Linktree</a> &nbsp/&nbsp
                <a href="mailto:sjagades@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                <a href="mailto:saimitheran06@gmail.com">Personal Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sai-mitheran/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/smj007/">GitHub</a>
              </p>

              <p>
                <b> I am actively seeking full-time Machine Learning Engineer and ML research opportunities starting in January 2024. </b>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/2023_pic.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/2023_pic.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <hr>
        
          <!-- News -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <div class="scrolling"
                    style="margin-top:10px;margin-bottom:20px;width:100%;overflow-y:scroll; height:70px;">
                    <ul>
                      <li>
                        09/23: Joining <a href="https://theairlab.org/">AirLab</a> to explore large-scale scene understanding!
                      </li> 
                      <li>
                        08/23: Teaching (assistant) <a href="https://courses.ece.cmu.edu/18290">18-290</a> (Signals and Systems) once more at CMU!
                      </li> 
                      <li>
                        07/23: Serving as a Reviewer for the <a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems"> IEEE Transactions on Neural Networks and Learning Systems</a>!
                      </li>  
                      <li>
                        06/23: Transforming Edge AI at <a href="https://latentai.com/">Latent AI</a> as an MLE intern for the summer!
                      </li>
                      <li>
                        05/23: Awarded the Sri. Janardhana Iyengar Memorial Award at NIT Trichy for the best academic performance in 2022!
                      </li>
                      <li>
                        02/23: <a href="https://ieeexplore.ieee.org/document/9944843">Paper</a> accepted at ICRA 2023!
                      </li>   
                      <li>
                        01/23: Teaching (assistant) <a href="https://courses.ece.cmu.edu/18290">18-290</a>, Signals and Systems at CMU!
                      </li> 
                      <li>
                        10/22: <a href="https://ieeexplore.ieee.org/document/9944843">Paper</a> accepted at IEEE Robotics and Automation Letters (RA-L)!
                      </li>  
                      <li>
                        09/22: Started a Research Assistantship at <a href="https://www.cylab.cmu.edu/">CyLab, CMU</a> in the Biometrics team!
                      </li> 
                      <li>
                        08/22: Teaching (assistant) <a href="https://courses.ece.cmu.edu/18794">18-794</a>, Pattern Recognition Theory at CMU!
                      </li> 
                      <li>
                        07/22: <a href="https://www.researchgate.net/publication/361951969_Rich_Feature_Distillation_with_Feature_Affinity_Module_for_Efficient_Image_Dehazing">Paper</a> accepted at <a href="https://www.journals.elsevier.com/optik">Optik</a>!
                      </li> 
                      <li>
                        05/22: <a href="https://www.researchgate.net/publication/361265311_Not_All_Lotteries_Are_Made_Equal">Paper</a> accepted at ICML 2022!
                      </li> 
                      <li>
                        03/22: Accepted to <a href="https://www.cmu.edu/">Carnegie Mellon University</a> as a full-time grad student!
                      </li> 
                      <li>
                        01/22: <a href="https://arxiv.org/abs/2201.11957">Paper</a> accepted at ICRA 2022!
                      </li>                    
                      <li>
                        01/22: Selected for <a href="https://sites.google.com/view/researchweek2022"> Research Week with Google Research, 2022</a>
                      </li>
                      <li>
                        12/21: Check out my <a href="https://linktr.ee/smj007"> Linktree</a>!
                      </li>
                      <li>
                        12/21: <a href="https://www.researchgate.net/publication/356815794_Audiomer_A_Convolutional_Transformer_for_Keyword_Spotting">Paper</a> accepted at AAAI 2022!
                      </li>  
                      <li>
                        11/21: Received an honorary mention and award for the <a 
                        href="https://aiforgood.itu.int/"> AI For Good </a> Challenge.
                      </li>
                    </ul>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
        
       <hr>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research and Experience</heading>
              
              <p>
                
                I collaborated with researchers at the <a href="http://www.labren.org/mm/">Medical Mechatronics Lab</a>, National University of Singapore, as a Research Assistant to work on Graph-based Deep Reasoning and Surgical Scene Understanding.
                As a Deep Learning Engineer at <a href = "https://aimonk.com/">AIMonk Labs Pvt. Ltd.</a> I worked in a team of five to build <a href = "https://neuralmarker.ai/">Neuralmarker</a>, transforming businesses with Computer Vision.
                I received a recommendation from <a href = "https://www.foxconn.com/en-us/">Foxconn</a> Country Head, Josh Foulger, to work with their Intelligent Systems Team on prototyping. 
             </p>
              
              <p>
                Previously, I was affiliated with the <a href="https://ed.iitm.ac.in/~raman/agcl/agcl.html"> Advanced Geometric Computing Lab</a> and the <a href="https://shakti.org.in/index.html">Shakti Group</a>, Indian Institute of Technology, Madras. I was supervised by <a href="https://ed.iitm.ac.in/~raman/">Dr. M. Ramanathan</a> and <a href="https://scholar.google.co.in/citations?user=pfG0_pEAAAAJ&hl=en">Dr. V. Kamakoti</a>, the Director of IIT-M.
                I've worked on Deep Generative Modelling of Real-time Wireless Communication Channels using UAVs, with <a href="https://www.nitt.edu/home/academics/departments/ece/faculty/gopi/">Dr. E.S Gopi</a>, Pattern Recognition Lab NIT Trichy, and <a href="https://sites.google.com/site/dushnalinjayakody/">Dr. Nalin Jayakody</a>, the Director of the Tomsk Infocomm Lab.
              
              </p>
<!--               
             <p>   
                My undergraduate research has primarily been focused on Generative Modelling, Graphics, 2D/3D Vision, and Medical Imaging. My ongoing research projects are either <b>currently being documented or are under review, the page will be updated soon</b>.
              </p> -->
              
             <p>
               Now that you've kept reading till here, check out the work done by my amazing research group at <a href="https://github.com/The-Learning-Machines">The Learning Machines</a>.
               </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vit.png" alt="vit" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2309.02617.pdf" id="">
                <papertitle>Compressing Vision Transformers for Low-Resource Visual Learning</papertitle>
              </a>
              <br>
              <a href="https://github.com/ey1678">Youn, Eric</a> and <a href="https://smj007.github.io/"><strong>Mitheran, Sai</strong></a> and <a href="https://github.com/sanjana-prabhu">Prabhu, Sanjana</a> and <a href="https://chensy7.github.io/">Chen, Siyuan</a>
              <br>
              <a href="https://arxiv.org/abs/2309.02617">arXiv</a> /
              <a href="https://arxiv.org/pdf/2309.02617.pdf">Paper</a> 
              <p></p>
              <p> Our work introduces a framework for compressing Vision Transformer models for efficient segmentation, with a focus on enabling deployment on resource-constrained devices like the NVIDIA Jetson Nano (4GB). Our approach combines structured pruning, distillation from a stronger teacher, and quantization strategies to significantly reduce memory usage and inference latency while maintaining high segmentation accuracy and mean IoU. This allows for the rapid deployment of Vision Transformers on the edge. </p>
            </td>
          </tr>       

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/architecture-1.png" alt="rethink" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9944843" id="">
                <papertitle>Rethinking Feature Extraction: Gradient-based Localized Feature Extraction for End-to-End Surgical Downstream Tasks</papertitle>
              </a>
              <br>
              <a href="https://github.com/PangWinnie0219">Pang, Winnie</a> and <a href="https://mobarakol.github.io/">Islam, Mobarakol</a> and <a href="https://smj007.github.io/"><strong>Mitheran, Sai</strong></a> and <a href="https://github.com/lalithjets">Seenivasan, Lalithkumar</a> and <a href="https://www.researchgate.net/profile/Xu-Mengya">Xu, Mengya</a> and <a href="https://scholar.google.com/citations?user=rcF7N44AAAAJ&hl=en">Ren, Hongliang</a>
              <br>
              <em>ICRA, 2023 and IEEE RA-L</em>
              <br>
              <a href="https://github.com/smj007/GradCAMDownstreamTask">Code</a> /
              <a href="https://ieeexplore.ieee.org/document/9944843">Paper</a>
              <p></p>
              <p>This work develops a detector-free gradient-based localized feature extraction approach that enables end-to-end model training for downstream surgical tasks such as report generation and tool-tissue interaction graph prediction. We eliminate the need for object detection or region proposal and feature extraction networks by extracting the features of interest from the discriminative regions in the feature map of the classification models. Here, the discriminative regions are localized using gradient-based localization techniques (e.g. Grad-CAM). We show that our proposed approaches enable the real-time deployment of end-to-end models for surgical downstream tasks.</p>
            </td>
          </tr>  
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/FA_dehaze.png" alt="dehaze" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.researchgate.net/publication/361951969_Rich_Feature_Distillation_with_Feature_Affinity_Module_for_Efficient_Image_Dehazing" id="">
                <papertitle>Rich Feature Distillation with Feature Affinity Module for Efficient Image Dehazing</papertitle>
              </a>
              <br>
              <a href="https://smj007.github.io/"><strong>Mitheran, Sai</strong></a> and <a href="https://github.com/anushrisuresh">Suresh, Anushri</a> and <a href="https://www.nitt.edu/home/academics/departments/ece/faculty/varun/">P Gopi, Varun</a>
              <br>
              <em>Optik, Elsevier</em>
              <br>
              <a href="https://github.com/smj007/FA_Dehaze">Code</a> /
              <a href="https://arxiv.org/abs/2207.11250">arXiv</a> /
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0030402622009433">Paper</a> 
              <p></p>
              <p> This work introduces a simple, lightweight, and efficient framework for single-image haze removal, exploiting rich ‚Äúdark-knowledge" information from a lightweight pre-trained super-resolution model via the notion of heterogeneous knowledge distillation. </p>
            </td>
          </tr>   
          
        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/lth.png" alt="lth" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2206.08175" id="">
                <papertitle>Not All Lotteries Are Made Equal</papertitle>
              </a>
              <br>
              <a href="https://github.com/ojus1">Sahu, Surya Kant</a> and <a href="https://smj007.github.io/"><strong>Mitheran, Sai</strong></a> and <a href="https://github.com/Daketey">Mahapatra, Ritul</a>
              <br>
              <em>ICML</em>, 2022 (HAET Workshop)
              <br>
              <a href="https://arxiv.org/abs/2206.08175">arXiv</a> /
              <a href="https://www.researchgate.net/publication/361265311_Not_All_Lotteries_Are_Made_Equal">Paper</a> 
              <p></p>
              <p>The Lottery Ticket Hypothesis (LTH) states that for a reasonably sized neural network, there exists a subnetwork within the same network that, when trained from the same initialization, yields no less performance than the dense counterpart. We investigate the effect of model size and the ease of finding winning tickets. Through this work, we show that winning tickets is in-fact, easier to find for smaller models.</p>
            </td>
          </tr>  
          
          
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gr_mtl.png" alt="gr_mtl" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2201.11957" id="">
                <papertitle>Global-Reasoned Multi-Task Learning Model for Surgical Scene Understanding</papertitle>
              </a>
              <br>
              <a href="https://github.com/lalithjets">Seenivasan, Lalithkumar*</a> and <a href="https://smj007.github.io/"><strong>Mitheran, Sai*</strong></a> and <a href="https://mobarakol.github.io/">Islam, Mobarakol</a> and <a href="https://scholar.google.com/citations?user=rcF7N44AAAAJ&hl=en">Ren, Hongliang</a>
              <br>
              <em>ICRA</em>, 2022 and <em>IEEE RA-L</em> <b>[SOTA, Endovis18]</b>
              <br>
              <a href="https://github.com/lalithjets/Global-reasoned-multi-task-model">Code</a> /
              <a href="https://arxiv.org/abs/2201.11957">arXiv</a> /
              <a href="https://ieeexplore.ieee.org/document/9695281">Paper</a>
              <p></p>
              <p>This paper introduces a globally-reasoned multi-task surgical scene understanding model capable of performing instrument segmentation and tool-tissue interaction detection.</p>
            </td>
          </tr>  
          
<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cGAN.png" alt="cgan" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="" id="">
                <papertitle>Data-driven Low Altitude UAV Channel Model using Generative Adversarial Networks</papertitle>
              </a>
              <br>
              <a href="https://smj007.github.io/"><strong>Mitheran, Sai</strong></a> and <a href="https://www.kaggle.com/vineeth1999">Raj, Vineeth</a> and <a href="https://sites.google.com/view/gopi-es/home">Gopi, E.S</a> and <a href="https://scholar.google.com/citations?user=ZrvkDdwAAAAJ&hl=en">Hydher, Hassaan</a> and <a href="https://sites.google.com/site/dushnalinjayakody/">Jayakody, DNK</a>
              <br>
              <em>Signals and Communication Technology, Springer </em><b>[TBR]</b>
              <br>
              <a href="https://github.com/smj007/LA-UAV">Code</a> 
              <p></p>
              <p>Low altitude UAV channel modeling in Urban, Suburban, Dense Urban, High rise Urban environments by conditioning a Generative Adversarial pipeline. </p>
            </td>
          </tr>  -->
         
    
<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Antenna_Beam.png" alt="cgan" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="" id="">
                <papertitle>Algorithmic Applications in Radar Systems</papertitle>
              </a>
              <br>
              <a href="https://smj007.github.io/"><strong>Mitheran, Sai</strong></a> and <a href="https://github.com/ram2091999">T N, Ram</a> and <a href="https://www.nitt.edu/home/academics/departments/ece/faculty/hemant/">K, Hemant</a>
              <br>
              <b>[TBR]</b>
              <br>
              <a href="https://smj007.github.io/">Paper</a> 
              <p></p>
              <p> We present an application-oriented survey of how algorithms from each paradigm, namely rule-based algorithms and intelligent learning-based algorithms, can present intuitive solutions to widen the scope for developing Massive MIMO technology and its broader usage for various tasks. </p>
            </td>
          </tr>       -->
          
<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ANS_blog-Nav.drawio.png" alt="nav" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="" id="">
                <papertitle>Moving Beyond Navigation with Active Neural SLAM</papertitle>
              </a>
              <br>
              <a href="https://smj007.github.io/"><strong>Mitheran, Sai</strong></a> and <a href="https://github.com/ojus1">Sahu, Surya Kant</a>
              <br>
              <b>[TBR]</b>
              <br>
              <a href="https://github.com/The-Learning-Machines">Code</a> 
              <p></p>
              <p>The ability to effectively harness autonomous control in real-world 3D environments largely depends on learning realistic navigation techniques for embodied agents. We propose ideas for Scene understanding using Active Neural SLAM, further synthesizing motion in complex scenes for VR and gaming. </p>
            </td>
          </tr>   -->
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/audiomer.png" alt="audiomer" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.researchgate.net/publication/356815794_Audiomer_A_Convolutional_Transformer_for_Keyword_Spotting" id="">
                <papertitle>Audiomer: A Convolutional Transformer for Keyword Spotting</papertitle>
              </a>
              <br>
              <a href="https://github.com/ojus1">Sahu, Surya Kant</a> and <a href="https://smj007.github.io/"><strong>Mitheran, Sai</strong></a> and <a href="https://github.com/thewhiteflower110">Kamdar, Juhi</a> and <a href="https://github.com/meetgandhi123">Gandhi, Meet</a>
              <br>
              <em>AAAI</em>, 2022 (DSTC10 Workshop) <b>[SOTA, Keyword Spotting]</b>
              <br>
              <a href="https://github.com/smj007/Audiomer-PyTorch">Code</a> /
              <a href="https://www.researchgate.net/publication/356815794_Audiomer_A_Convolutional_Transformer_for_Keyword_Spotting">Paper</a> 
              <p></p>
              <p>In this work, we introduce an architecture, Audiomer, where we combine 1D Residual Networks with Performer Attention to achieve state-of-the-art performance in Keyword Spotting with raw audio waveforms, out-performing all previous methods while also being computationally cheaper and parameter-efficient.</p>
            </td>
          </tr>   
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SBR_Task.png" alt="SBR" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2107.01516" id="">
                <papertitle>Introducing Self-Attention to Target Attentive Graph Neural Networks</papertitle>
              </a>
              <br>
              <a href="https://smj007.github.io/"><strong>Mitheran, Sai</strong></a> and <a href="https://java-abhinav07.github.io/">Java, Abhinav</a> and <a href="https://github.com/ojus1">Sahu, Surya Kant</a> and <a href="https://github.com/arshadshk">Shaikh, Arshad</a>
              <br>
              <em>AISP</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2107.01516.pdf">Paper</a> /
              <a href="https://github.com/The-Learning-Machines/SBR">Code</a> /
              <a href="https://arxiv.org/abs/2107.01516">arXiv</a>
              <p></p>
              <p>We propose using a Transformer in combination with a target attentive GNN, which allows richer Representation Learning. We outperform the existing methods on real-world benchmark datasets.</p>
            </td>
          </tr> 
     
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/wgv.png" alt="gr_mtl" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="" id="">
                <papertitle>User-Friendly Waveguide Modes Visualiser</papertitle>
              </a>
              <br>
              <a href="https://smj007.github.io/"><strong>Mitheran, Sai</strong></a> and <a href="https://github.com/ram2091999">T N, Ram</a> and <a href="https://www.nitt.edu/home/academics/departments/ece/faculty/professors/raghavan/">S, Raghavan</a>
              <br>
              <em>IEEE Microwave Magazine</em> 2022, <em>Microwaves 101</em>, <em>Recent Trends on Metamaterial Antennas for Wireless Applications and Deep Learning Techniques, 2021</em>
              <br>
              <a href="https://ieeexplore.ieee.org/document/9748896">Paper</a> /
              <a href="https://share.streamlit.io/ram2091999/tl-modes-visualiser/app.py">Application</a> /
              <a href="https://www.microwaves101.com/encyclopedias/national-institute-of-technology-trichy#:~:text=to%20Microwave%20Engineering%22-,new%20for%20march%202021%3A,-Two%20undergraduate%20students">Microwaves 101</a>
              <p></p>
              <p>This article presents the procedure and results of a web application made to visualize field lines of Electric and Magnetic waves inside a waveguide. We propose a first-of-the-kind Graphical User Interface for waveguide visualization as a public resource. </p>
            </td>
          </tr>  
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hand_drawn.png" alt="hand_drawn" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://bharadwaj-manda.netlify.app/files/pdf/CADSketchNet_accepted_version.pdf" id="MCG_journal">
                <papertitle>'CADSketchNet' - An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks</papertitle>
              </a>
              
              <br>
              <a href="https://bharadwaj-manda.netlify.app/">Manda, Bharadwaj</a> and <a href="https://www.linkedin.com/in/shubham-dhayarkar-a16a75153/?originalSubdomain=in">Dhayarkar, Shubham</a> and <a href="https://smj007.github.io/"><strong>Mitheran, Sai</strong></a> and <a href="https://vkviekash-homepage.github.io/">V.K, Viekash</a> and <a href="https://ed.iitm.ac.in/~raman/">Muthuganapathy, Ramanathan</a>
              <br>
              <em>3DOR</em>, 2021 and <em>Computers & Graphics</em> Journal
              <br>
              <a href="https://bharadwaj-manda.github.io/CADSketchNet/">Project Page</a> /
              <a href="https://bharadwaj-manda.netlify.app/files/pdf/CADSketchNet_accepted_version.pdf">Paper</a>
              <p></p>
              <p>We introduce the CADSketchNet dataset, an annotated collection of sketches of 3D CAD models, which is intended to enhance the research on developing AI-enabled search engines for 3D CAD models. We also evaluate the performance of various retrieval systems. Many experimental models are constructed and tested on CADSketchNet.</p>
            </td>
          </tr>
          
        </tbody></table> 
            
        <hr>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
           <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/iclr.png" width="160" style="border-style: none"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://iclr.cc/Conferences/2021">Student Volunteer, ICLR 2021</a>
            </td>
          </tr>
          <tr>
             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ICML.jpeg" width="160" style="border-style: none" alt="icml">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://icml.cc/Conferences/2021">Student Volunteer, ICML 2021</a>
            </td>
          </tr>
        </tbody></table>
        
        <hr>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td>
            <heading>Affiliations (Upto Date)</heading>
            </td>
          </tr>
          </tbody></table>
        <table align="center">
        <tbody><tr>
          
            <td width="16%" align="center">
                <a href="https://theairlab.org/" target="_blank">
                <img style="width:120px" src="images/the_air_lab_at_carnegie_mellon_university_logo.jpeg"></a>&nbsp; &nbsp;
            </td>
          
            <td width="17%" align="center">
              <a href="https://latentai.com/" target="_blank">
              <img style="width:120px" src="images/LatentAI_logo-ai.png"></a>&nbsp; &nbsp;
          </td>
            <td width="16%" align="center">
                <a href="https://www.cylab.cmu.edu/" target="_blank">
                <img style="width:140px" src="images/cylab-original-stacked.png"></a>&nbsp; &nbsp;
            </td>
          
            <td width="17%" align="center">
                <a href="https://virtualhumans.mpi-inf.mpg.de/" target="_blank">
                <img style="width:150px" src="images/mpiinf.png"></a>&nbsp; &nbsp;
            </td>
  
        </tr>
        <tr>
            <td width="17%" align="center">
              <a href="https://www.iitm.ac.in/" target="_blank">
              <img style="width:120px" src="images/iitm.png"></a>&nbsp; &nbsp;
          </td>
            <td width="17%" align="center">
                <a href="https://aimonk.com/" target="_blank">
                <img style="width:140px" src="images/aimonk.png"></a>&nbsp; &nbsp;
              
            <td width="17%" align="center">
              <a href="https://www.foxconn.com/en-us/" target="_blank">
              <img style="width:150px" src="images/foxconn.png"></a>&nbsp; &nbsp;
            </td>
          <td width="17%" align="center">
            <a href="http://www.labren.org/mm/" target="_blank">
            <img style="width:120px" src="images/NUS.jpg"></a>&nbsp; &nbsp;
          </td>  
      </td>

        </tr>
    </tbody></table>

      <hr>
        
       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td>
            <heading>Volunteering and Initiatives</heading>
            </td>
          </tr>
          </tbody></table>
        <table align="center">
        <tbody><tr>
            <td width="16%" align="center">
                <a href="https://www.uandi.org.in/" target="_blank">
                <img style="width:120px" src="images/Screenshot 2021-11-21 104028.png"></a>&nbsp; &nbsp;
            </td>
            <td width="17%" align="center">
                <a href="https://www.instagram.com/hopenittrichy/?hl=en" target="_blank">
                <img style="width:150px" src="images/hope_NITT.jpg"></a>&nbsp; &nbsp;
            </td>

            <td width="17%" align="center">
              <a href="https://www.linkedin.com/company/hope-organization-coimbatore/?originalSubdomain=in" target="_blank">
              <img style="width:120px" src="images/hope_org.png"></a>&nbsp; &nbsp;
          </td>
            <td width="17%" align="center">
                <a href="https://www.facebook.com/gaiaorg01/" target="_blank">
                <img style="width:140px" src="images/GAIA.jpg"></a>&nbsp; &nbsp;
            </td>
  
        </tr>

    </tbody></table>        
    
        <br>
        
        <table width="30%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="30%" align="center">
              <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=gGY-SaXu-h7sP8ZlmsWwDPlJFS11Va2ulIKlJ0km61c&cl=ffffff&w=a"></script>
            </td>
            </tr>
            
        </tbody></table>
        
        
                <footer>
                  <p>
                Template Credits : <a href="https://github.com/jonbarron/website">Jon Barron's Website</a>. 
                  </p></footer>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

</body>
</html>
